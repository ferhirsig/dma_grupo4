{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "import math\n",
    "from sklearn.preprocessing import LabelBinarizer\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cargar imágenes y hacer PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cargo los dos numpy arrays que representan los datasets de train y test, estan en la carpeta datasets del repositorio\n",
    "train = np.load(\"datasets/train.npy\")\n",
    "test = np.load(\"datasets/test.npy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def displayData(data, *width):\n",
    "    '''\n",
    "    Display data in a 2-dimensional grid\n",
    "    '''\n",
    "\n",
    "    # Set ex_width\n",
    "    if width:\n",
    "        ex_width = width[0]\n",
    "    else:\n",
    "        ex_width = int(np.sqrt(np.shape(data)[1]));\n",
    "\n",
    "    # Compute dimensions\n",
    "    (n_examples, n) = np.shape(data);\n",
    "    ex_height = int(n / ex_width);\n",
    "    n_rows = int(np.floor(np.sqrt(n_examples)));\n",
    "    n_cols = int(np.ceil(n_examples / n_rows));\n",
    "\n",
    "    # Set padding\n",
    "    pad = 1;\n",
    "\n",
    "    # Core\n",
    "    grid = np.zeros( (pad + n_rows * (ex_height + pad),\n",
    "                                pad + n_cols * (ex_width + pad)) );    \n",
    "    cur = 0; # current example\n",
    "    for j in range(0, n_rows):\n",
    "        if cur >= n_examples: break;\n",
    "        for i in range(0, n_cols):\n",
    "            if cur >= n_examples: break;\n",
    "            max_val = np.max(np.abs(data[cur, :]))\n",
    "            from_row = pad + j * (ex_height + pad); to_row = from_row + ex_height\n",
    "            from_col = pad + i * (ex_width + pad); to_col = from_col + ex_width\n",
    "            grid[from_row:to_row,from_col:to_col] = \\\n",
    "                data[cur, :].reshape( (ex_height, ex_width) ) / max_val\n",
    "            cur += 1\n",
    "        \n",
    "\n",
    "    # Display data\n",
    "    if(n_examples<100):\n",
    "        fig = plt.figure()\n",
    "    else:\n",
    "        fig = plt.figure(figsize=(n_examples/10,n_examples/20))\n",
    "    ax = fig.add_axes([0, 0, 1, 1])\n",
    "    ax.imshow((-1)*grid, extent=[0, 1, 0, 1], cmap='Greys')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displayData(train[:201],100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displayData(test[:201],100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# funcion que devuelve el PCA de un dataset, es decir, por cada foto (fila) va a devolver <n_componentes> cantidad de columnas.\n",
    "# esto es lo que se usa de entrada\n",
    "def pca_faces(n_componentes, dataset_train, dataset_test):\n",
    "    pca = PCA(n_components= n_componentes)\n",
    "    pca.fit(dataset_train)\n",
    "    U = pca.components_\n",
    "    Z_train = pca.transform(dataset_train)\n",
    "    Z_test = pca.transform(dataset_test)\n",
    "    \n",
    "    return Z_train, Z_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hago el pca para las fotos de train\n",
    "\n",
    "pca_train, pca_test = pca_faces(30, train, test)\n",
    "\n",
    "print(pca_test.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_train_ contiene los nombres de la persona de cada foto EN ORDEN como aparecen en el dataset (ver mas arriba donde estan mostradas las fotos)\n",
    "\n",
    "X_train = pca_train\n",
    "X_test = pca_test\n",
    "y_train_ = ['nestor', 'claudia', 'oscar', 'eduardo', 'andres', 'eduardo', 'lujan', 'claudia', 'maira', 'eduardo', 'marcelo',\n",
    "           'marcelo t.','eduardo', 'marcelo', 'eduardo', 'marcelo', 'eduardo', 'maira', 'silvia', 'marisa', 'sebastian', 'marcelo t.',\n",
    "           'geronimo', 'eduardo', 'oscar', 'lujan', 'jiang', 'eduardo', 'maira', 'eduardo', 'sebastian', 'nestor', 'fernanda',\n",
    "           'hernan', 'geronimo', 'eduardo', 'elemir', 'julieta', 'silvia', 'eduardo', 'josefina', 'marcelo t.', 'marcelo t.', 'hernan',\n",
    "           'julieta', 'josefina', 'marcelo t.', 'lujan', 'hernan', 'jiang', 'nestor', 'eduardo', 'jiang', 'marcelo t.', 'eduardo',\n",
    "           'joaquin', 'hernan', 'jiang', 'julieta', 'marisa', 'eduardo', 'eduardo', 'fernanda', 'marcelo', 'jiang', 'nestor',\n",
    "           'josefina', 'eduardo', 'lujan', 'rodrigo', 'rodrigo', 'sebastian', 'marcelo', 'andres', 'marcelo', 'julieta', 'hernan',\n",
    "           'rodrigo', 'maribel', 'eduardo', 'fernanda', 'fernanda', 'rodrigo', 'marisa', 'maribel', 'eduardo', 'jiang', 'eduardo',\n",
    "           'julieta', 'silvia', 'eduardo', 'eduardo', 'marcelo t.', 'marcelo t.', 'fernanda', 'joaquin', 'lujan', 'jiang', 'eduardo',\n",
    "           'andres', 'maira', 'nestor', 'jiang', 'claudia', 'nestor', 'elemir', 'hernan', 'maira', 'hernan', 'eduardo', \n",
    "           'andres', 'marisa', 'eduardo', 'hernan', 'rodrigo', 'eduardo', 'elemir', 'josefina', 'andres', 'marcelo t.', 'josefina']\n",
    "\n",
    "y_test_ = ['claudia', 'sebastian', 'nestor', 'andres', 'jiang', 'maribel', 'hernan', 'marcelo', 'marisa', 'rodrigo',\n",
    "           'elemir', 'josefina', 'julieta', 'geronimo', 'eduardo', 'fernanda', 'lujan', 'marcelo t.', 'silvia',\n",
    "           'joaquin', 'oscar']\n",
    "\n",
    "# para poder usar esta lista de nombres, debo meterlas como un array de 0s y 1s.\n",
    "# en la celda de abajo se ve mas claro pero aca esta la transformacion hecha\n",
    "y_train = LabelBinarizer().fit_transform(y_train_)\n",
    "y_test = LabelBinarizer().fit_transform(y_test_)\n",
    "\n",
    "index = 12\n",
    "\n",
    "y_test = np.insert(y_test, index, values=np.zeros(y_test.shape[0]), axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imprimo cada elemento del vector de nombres con su correspondiente array de 0 y 1.\n",
    "# por ejemplo el array que tenga un 1 en la posicion 2 significa \"claudia\"\n",
    "for i in range(10):\n",
    "    print(f\"{y_train_[i]}:{y_train[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(21):\n",
    "    print(f\"{y_test_[i]}:{y_test[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.Series(y_train_).value_counts()\n",
    "pd.Series(y_test_).value_counts()\n",
    "## esto es para ver cuantas fotos hay de cada persona. Si hay menos fotos va a tener menos chance de reconocer a cada persona.\n",
    "# Nuestro dataset le faltan fotos, por eso hay menos fotos por persona.\n",
    "\n",
    "y_test.shape\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento de la red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## aca arranca todo el script, marco con ####### las partes que modifique del original\n",
    "\n",
    "\n",
    "def func_eval(fname, x):\n",
    "    match fname:\n",
    "        case \"purelin\":\n",
    "            y = x\n",
    "        case \"logsig\":\n",
    "            y = 1.0 / ( 1.0 + np.exp(-x) )\n",
    "        case \"tansig\":\n",
    "            y = 2.0 / ( 1.0 + math.exp(-2.0*x) ) - 1.0\n",
    "    return y\n",
    "\n",
    "func_eval_vec = np.vectorize(func_eval)\n",
    "\n",
    "def deriv_eval(fname, y):  \n",
    "    match fname:\n",
    "        case \"purelin\":\n",
    "            d = 1.0\n",
    "        case \"logsig\":\n",
    "            d = y*(1.0-y)\n",
    "        case \"tansig\":\n",
    "            d = 1.0 - y*y\n",
    "    return d\n",
    "\n",
    "deriv_eval_vec = np.vectorize(deriv_eval)\n",
    "\n",
    "# La entrada ahora va a ser X_train que es la matriz  con el PCA de las fotos de train\n",
    "###############################################\n",
    "entrada =  X_train \n",
    "###############################################\n",
    "\n",
    "\n",
    "# La salida ahora es y_train, un array de 0s y 1s por cada foto, indicando a quien pertenece como vimos arriba.\n",
    "###############################################\n",
    "salida = y_train\n",
    "###############################################\n",
    "\n",
    "\n",
    "# Paso las listas a numpy\n",
    "X = np.array(entrada)\n",
    "Y = np.array(salida)\n",
    "\n",
    "filas_qty = len(X)\n",
    "###########################\n",
    "# Cantidad de neuronas de entradas: 1 por Componente principal\n",
    "input_size = X.shape[1]   # 1 por CP\n",
    "###########################\n",
    "\n",
    "\n",
    "hidden_size = 10  # neuronas capa oculta\n",
    "\n",
    "# Cantidad de neuronas de salida: 1 por persona en train\n",
    "###########################\n",
    "output_size = Y.shape[1] \n",
    "###########################\n",
    "\n",
    "# defino las funciones de activacion de cada capa\n",
    "hidden_FUNC = 'logsig'  # uso la logistica\n",
    "\n",
    "###############################################\n",
    "output_FUNC = 'logsig'  # uso la logistica\n",
    "###############################################\n",
    "\n",
    "\n",
    "# incializo los graficos\n",
    "#grafico = perceptron_plot(X, np.array(salida), 0.0)\n",
    "\n",
    "# Incializo las matrices de pesos azarosamente\n",
    "# W1 son los pesos que van del input a la capa oculta\n",
    "# W2 son los pesos que van de la capa oculta a la capa de salida\n",
    "np.random.seed(1021) #mi querida random seed para que las corridas sean reproducibles\n",
    "W1 = np.random.uniform(-0.5, 0.5, [hidden_size, input_size])\n",
    "X01 = np.random.uniform(-0.5, 0.5, [hidden_size, 1] )\n",
    "W2 = np.random.uniform(-0.5, 0.5, [output_size, hidden_size])\n",
    "X02 = np.random.uniform(-0.5, 0.5, [output_size, 1] )\n",
    "\n",
    "# Avanzo la red, forward\n",
    "# para TODOS los X al mismo tiempo ! \n",
    "#  @ hace el producto de una matrix por un vector_columna\n",
    "hidden_estimulos = W1 @ X.T + X01\n",
    "hidden_salidas = func_eval_vec(hidden_FUNC, hidden_estimulos)\n",
    "output_estimulos = W2 @ hidden_salidas + X02\n",
    "output_salidas = func_eval_vec(output_FUNC, output_estimulos)\n",
    "\n",
    "# calculo el error promedi general de TODOS los X\n",
    "Error = np.mean( (Y.T - output_salidas)**2 )\n",
    "print(f\"Error inicial {Error}\")\n",
    "\n",
    "# Inicializo\n",
    "epoch_limit = 2000    # para terminar si no converge\n",
    "Error_umbral = 1.0e-06\n",
    "learning_rate = 0.2\n",
    "Error_last = 10    # lo debo poner algo dist a 0 la primera vez\n",
    "epoch = 0\n",
    "\n",
    "while ( math.fabs(Error_last-Error)>Error_umbral and (epoch < epoch_limit)):\n",
    "    epoch += 1\n",
    "    Error_last = Error\n",
    "    \n",
    "    # recorro siempre TODA la entrada\n",
    "    for fila in range(filas_qty): #para cada input x_sub_fila del vector X\n",
    "        # propagar el x hacia adelante\n",
    "        hidden_estimulos = W1 @ X[fila:fila+1, :].T + X01\n",
    "        hidden_salidas = func_eval_vec(hidden_FUNC, hidden_estimulos)\n",
    "        output_estimulos = W2 @ hidden_salidas + X02\n",
    "        output_salidas = func_eval_vec(output_FUNC, output_estimulos)\n",
    "        \n",
    "        # calculo los errores en la capa hidden y la capa output\n",
    "        ErrorSalida = Y[fila:fila+1,:].T - output_salidas\n",
    "        # output_delta es un solo numero\n",
    "        output_delta = ErrorSalida * deriv_eval_vec(output_FUNC, output_salidas)\n",
    "        # hidden_delta es un vector columna\n",
    "        hidden_delta = deriv_eval_vec(hidden_FUNC, hidden_salidas)*(W2.T @ output_delta)\n",
    "\n",
    "        # ya tengo los errores que comete cada capa\n",
    "        # corregir matrices de pesos, voy hacia atras\n",
    "        # backpropagation\n",
    "        W1 = W1 + learning_rate * (hidden_delta @ X[fila:fila+1, :] )\n",
    "        X01 = X01 + learning_rate * hidden_delta\n",
    "        W2 = W2 + learning_rate * (output_delta @ hidden_salidas.T)\n",
    "        X02 = X02 + learning_rate * output_delta\n",
    "\n",
    "    # ya recalcule las matrices de pesos\n",
    "    # ahora avanzo la red, feed-forward\n",
    "    hidden_estimulos = W1 @ X.T + X01\n",
    "    hidden_salidas = func_eval_vec(hidden_FUNC, hidden_estimulos)\n",
    "    output_estimulos = W2 @ hidden_salidas + X02\n",
    "    output_salidas = func_eval_vec(output_FUNC, output_estimulos)\n",
    "    print(hidden_estimulos.shape)\n",
    "    # calculo el error promedio general de TODOS los X\n",
    "    Error= np.mean( (Y.T - output_salidas)**2 )\n",
    "    \n",
    "    # Imprimo el error en cada epoch\n",
    "    ###############################################\n",
    "    print(f\"epoch: {epoch} - error: {Error}\")\n",
    "    ###############################################"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicciones"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "prediccion_train = []\n",
    "personas = np.unique(y_train_)\n",
    "\n",
    "for fila in range(len(y_train_)):\n",
    "\n",
    "    hidden_estimulos_predict = W1 @ X[fila:fila+1, :].T + X01\n",
    "    hidden_salidas_predict = func_eval_vec(hidden_FUNC, hidden_estimulos_predict)\n",
    "    output_estimulos_predict = W2 @ hidden_salidas_predict + X02\n",
    "    output_salidas_predict = func_eval_vec(output_FUNC, output_estimulos_predict)\n",
    "    prediccion_train.append(np.argmax(output_salidas_predict))\n",
    "\n",
    "prediccion_train = np.array(prediccion_train)\n",
    "y_train_ = np.array(y_train_)\n",
    "aciertos_train = []\n",
    "cant_aciertos_train = 0\n",
    "for indice in range(len(y_train_)):\n",
    "    aciertos_train.append(personas[prediccion_train[indice]])\n",
    "\n",
    "for x, y in zip(aciertos_train, y_train_):\n",
    "    if x == y:\n",
    "        cant_aciertos_train += 1\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"Cantidad de aciertos: \", cant_aciertos_train)\n",
    "print(\"Accuracy del training:\", round(cant_aciertos_train/len(aciertos_train),3))\n",
    "print(\"Real - Predicción\")\n",
    "for indice in range(len(aciertos_train)):\n",
    "    print(y_train_[indice], \" - \", aciertos_train[indice])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_t = np.array(X_test)\n",
    "prediccion_test = []\n",
    "\n",
    "for fila in range(len(y_test_)):\n",
    "\n",
    "    hidden_estimulos_predict = W1 @ X_t[fila:fila+1, :].T + X01\n",
    "    hidden_salidas_predict = func_eval_vec(hidden_FUNC, hidden_estimulos_predict)\n",
    "    output_estimulos_predict = W2 @ hidden_salidas_predict + X02\n",
    "    output_salidas_predict = func_eval_vec(output_FUNC, output_estimulos_predict)\n",
    "    prediccion_test.append(np.argmax(output_salidas_predict))\n",
    "\n",
    "prediccion_test = np.array(prediccion_test)\n",
    "y_test_ = np.array(y_test_)\n",
    "aciertos_test = []\n",
    "cant_aciertos_test = 0\n",
    "for indice in range(len(y_test_)):\n",
    "    aciertos_test.append(personas[prediccion_test[indice]])\n",
    "\n",
    "for x, y in zip(aciertos_test, y_test_):\n",
    "    if x == y:\n",
    "        cant_aciertos_test += 1\n",
    "        \n",
    "print(\"Cantidad de aciertos: \", cant_aciertos_test)\n",
    "print(\"Accuracy del training:\", round(cant_aciertos_test/len(aciertos_test),3))\n",
    "print(\"Real - Predicción\")\n",
    "for indice in range(len(aciertos_test)):\n",
    "    print(y_test_[indice], \" - \", aciertos_test[indice])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorFlow and tf.keras\n",
    "import tensorflow as tf\n",
    "\n",
    "# Helper libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creación de la red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "escala = np.max([X_train.max(), X_test.max()])\n",
    "\n",
    "train_tensor = X_train / escala\n",
    "test_tensor = X_test / escala\n",
    "\n",
    "cant_personas = len(np.unique(y_train_))\n",
    "\n",
    "# capa oculta con 10 neuronas\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(30, 1)),\n",
    "    tf.keras.layers.Dense(10, activation='relu'),\n",
    "    tf.keras.layers.Dense(cant_personas)\n",
    "])\n",
    "\n",
    "model.compile(optimizer = tf.keras.optimizers.experimental.SGD(learning_rate=learning_rate),\n",
    "              loss = tf.keras.losses.MeanSquaredError(),\n",
    "              metrics=['accuracy'])\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento de la red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train_tensor, y_train, epochs=epoch_limit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(test_tensor,  y_test, verbose=2)\n",
    "\n",
    "print('\\nTest accuracy:', test_acc)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicciones Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se agrega \"Maira\" debido a que no hay una foto suya en los datos de test.\n",
    "predictions = model.predict(test_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aciertos = []\n",
    "cant_aciertos = 0\n",
    "for indice in range(len(y_test_)):\n",
    "    aciertos.append(personas[predictions[indice,:].argmax()])\n",
    "\n",
    "for x, y in zip(aciertos, y_test_):\n",
    "    if x == y:\n",
    "        cant_aciertos += 1\n",
    "        \n",
    "    \n",
    "\n",
    "print(\"Cantidad de aciertos: \",cant_aciertos)\n",
    "print(\"Cantidad fotos de testing: \",len(y_test_))\n",
    "print(\"Accuracy: \", round(cant_aciertos/len(y_test_), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Real - Predicción\")\n",
    "for indice in range(len(aciertos)):\n",
    "    print(y_test_[indice], \" - \", aciertos[indice])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dma-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
